{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from types import MethodType\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pyarrow import json_\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "COLLECTION_NAME_SUMM = 'timescale_WA_SC_v2'\n",
    "\n",
    "# Set up the logger\n",
    "logging.basicConfig(level=logging.INFO)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### LLMs\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ[\"API_KEY\"]\n",
    "\n",
    "LLAMA_3B_NAME = 'llama3.2'\n",
    "DEEPSEEK_1_5B_NAME = 'deepseek-r1:1.5b'\n",
    "\n",
    "small_llm = ChatOllama(model=LLAMA_3B_NAME, temperature=0.)\n",
    "# big_llm = ChatOllama(model=LLAMA_3B_NAME, temperature=0.)\n",
    "big_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    timeout=None\n",
    ")\n",
    "ret_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    timeout=None\n",
    ")\n",
    "\n",
    "# embed_model = OllamaEmbeddings(model=LLAMA_3B_NAME)\n",
    "embed_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "chunk_embed_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Load Data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_FILE = \"../data/WhatsAppCleaned/WhatsAppCombined.tsv\"\n",
    "\n",
    "def add_context(chat_df, col_to_cat='MESSAGE', new_col_name='CONTEXTUALIZED_MESSAGE', context_len=3, author_col='SENDER', date_col=None):\n",
    "  temp_col = col_to_cat+'_TMP'\n",
    "  chat_df[temp_col] = chat_df[author_col] + ' ~ ' + chat_df[col_to_cat] if not date_col else \\\n",
    "    chat_df[author_col] + ' @ ' + chat_df[date_col].dt.strftime('%A %B %d, %Y %H:%M') + ' ~ ' + chat_df[col_to_cat]\n",
    "\n",
    "  neg_cols_added = [f'{temp_col}_neg_{i}' for i in range(1, 1 + context_len)]\n",
    "  plus_cols_added = [f'{temp_col}_plus_{i}' for i in range(1, 1 + context_len)]\n",
    "\n",
    "  for i in range(1, context_len + 1):\n",
    "    chat_df[f'{temp_col}_plus_{i}'] = chat_df[temp_col].shift(-i)\n",
    "    chat_df[f'{temp_col}_neg_{i}'] = chat_df[temp_col].shift(i)\n",
    "\n",
    "  chat_df[new_col_name] = chat_df[[*neg_cols_added, temp_col, *plus_cols_added]].fillna('').agg('\\n'.join, axis=1).str.strip()\n",
    "  chat_df.drop(columns=[temp_col, *neg_cols_added, *plus_cols_added], inplace=True)\n",
    "  return chat_df\n",
    "\n",
    "\n",
    "data = pd.read_csv(DATA_FILE, sep='\\t', parse_dates=['DATETIME'])\n",
    "print(data.shape)\n",
    "data.dropna(inplace=True)\n",
    "print(data.shape)\n",
    "data = add_context(data, col_to_cat='MESSAGE', new_col_name='CONTEXTUALIZED_MESSAGE', context_len=0, date_col='DATETIME')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vectorstore"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text_splitter = SemanticChunker(\n",
    "  embed_model,\n",
    "  add_start_index=True,\n",
    "  breakpoint_threshold_type='percentile',\n",
    "  breakpoint_threshold_amount=90.,\n",
    "  min_chunk_size=3\n",
    ")\n",
    "\n",
    "def split_text_w_indices(\n",
    "    self,\n",
    "    text: str,\n",
    "    join_char: str = '\\n'\n",
    "):\n",
    "    start_indices = [0]\n",
    "\n",
    "    # Splitting the essay (by default on '.', '?', and '!')\n",
    "    single_sentences_list = re.split(self.sentence_split_regex, text)\n",
    "\n",
    "    # having len(single_sentences_list) == 1 would cause the following\n",
    "    # np.percentile to fail.\n",
    "    if len(single_sentences_list) == 1:\n",
    "        return single_sentences_list, start_indices\n",
    "    # similarly, the following np.gradient would fail\n",
    "    if (\n",
    "        self.breakpoint_threshold_type == \"gradient\"\n",
    "        and len(single_sentences_list) == 2\n",
    "    ):\n",
    "        return single_sentences_list, start_indices\n",
    "    distances, sentences = self._calculate_sentence_distances(single_sentences_list)\n",
    "    if self.number_of_chunks is not None:\n",
    "        breakpoint_distance_threshold = self._threshold_from_clusters(distances)\n",
    "        breakpoint_array = distances\n",
    "    else:\n",
    "        (\n",
    "            breakpoint_distance_threshold,\n",
    "            breakpoint_array,\n",
    "        ) = self._calculate_breakpoint_threshold(distances)\n",
    "\n",
    "    indices_above_thresh = [\n",
    "        i\n",
    "        for i, x in enumerate(breakpoint_array)\n",
    "        if x > breakpoint_distance_threshold\n",
    "    ]\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    # Iterate through the breakpoints to slice the sentences\n",
    "    for index in indices_above_thresh:\n",
    "        # The end index is the current breakpoint\n",
    "        end_index = index\n",
    "\n",
    "        # Slice the sentence_dicts from the current start index to the end index\n",
    "        group = sentences[start_indices[-1] : end_index + 1]\n",
    "        combined_text = join_char.join([d[\"sentence\"] for d in group])\n",
    "        # If specified, merge together small chunks.\n",
    "        if (\n",
    "            self.min_chunk_size is not None\n",
    "            and len(combined_text) < self.min_chunk_size\n",
    "        ):\n",
    "            continue\n",
    "        chunks.append(combined_text)\n",
    "\n",
    "        # Update the start index for the next group\n",
    "        start_indices.append(index + 1)\n",
    "\n",
    "    # The last group, if any sentences remain\n",
    "    if start_indices[-1] < len(sentences):\n",
    "        combined_text = join_char.join([d[\"sentence\"] for d in sentences[start_indices[-1]:]])\n",
    "        chunks.append(combined_text)\n",
    "    return chunks, start_indices\n",
    "\n",
    "text_splitter.split_text_w_indices = MethodType(split_text_w_indices, text_splitter)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SummWithReasoning(BaseModel):\n",
    "    think: str = Field(description='Think through how to summarize the messages')\n",
    "    summary: str = Field(description=\"The final summary\")\n",
    "\n",
    "# Chain\n",
    "summ_chain = big_llm.with_structured_output(SummWithReasoning)#, method='json_schema')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from timescale_vector import client\n",
    "\n",
    "def create_uuid2(datetime_obj):\n",
    "  if datetime_obj is None:\n",
    "    return None\n",
    "  uuid = client.uuid_from_time(datetime_obj.tz_localize('US/Pacific'))\n",
    "  return str(uuid)\n",
    "\n",
    "def create_date(dt):\n",
    "    if dt is None:\n",
    "        return None\n",
    "\n",
    "    # Extract relevant information\n",
    "    tz_info = dt.tz_localize('US/Pacific').utcoffset()\n",
    "    tz_str = f'{\"+\" if tz_info.days >= 0 else \"-\"}{np.abs(24*tz_info.days+tz_info.seconds//3600):02}{((tz_info.seconds%3600)//60):02}'\n",
    "    # Create a formatted string for the timestamptz in PostgreSQL format\n",
    "    timestamp_tz_str = (\n",
    "        f\"{dt.year}-{dt.month:02}-{dt.day:02} {dt.hour:02}:{dt.minute:02}:{dt.second:02}{tz_str}\"\n",
    "    )\n",
    "    return timestamp_tz_str\n",
    "\n",
    "def extract_metadata_summarize(row, context_len):\n",
    "  metadata = dict()\n",
    "  metadata[\"id\"] = create_uuid2(row[\"DATETIME\"])\n",
    "  metadata[\"MSG_ID\"] = row[\"MSG_ID\"]\n",
    "  metadata[\"DATETIME\"] = create_date(row[\"DATETIME\"])\n",
    "  metadata[\"CONTEXT_LEN\"] = context_len\n",
    "  metadata[\"PLATFORM\"] = row[\"PLATFORM\"]\n",
    "  metadata[\"CHAT\"] = row[\"CHAT\"]\n",
    "\n",
    "  return metadata"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "SUMM_MSGS_PROMPT = \"\"\"Please summarize the following messages in at most three sentences, focusing on this message thread's unique events and its participants.\n",
    "NEVER write polite phrases like \"sure thing\" or \"happy to help\" that would make the system reading the summary believe that it was written by a large language model.\n",
    "\n",
    "\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "docs = []\n",
    "\n",
    "text_splitter.sentence_split_regex = r'\\n\\n'\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR, force=True)\n",
    "for _, df in data.groupby(['PLATFORM', 'CHAT']):\n",
    "  df_groups, df_indices = text_splitter.split_text_w_indices(\n",
    "    '\\n\\n'.join(df['CONTEXTUALIZED_MESSAGE'])\n",
    "  )\n",
    "  df_indices.append(df.shape[0])\n",
    "\n",
    "  for i, group_str in tqdm(enumerate(df_groups), total=len(df_groups)):\n",
    "    st_idx = df_indices[i]\n",
    "    end_idx = df_indices[i+1]\n",
    "\n",
    "    summ_response = summ_chain.invoke(SUMM_MSGS_PROMPT+group_str)\n",
    "\n",
    "    if summ_response and summ_response.summary:\n",
    "      docs.append(\n",
    "        Document(\n",
    "          page_content=summ_response.summary,\n",
    "          metadata=extract_metadata_summarize(df.iloc[st_idx, :], end_idx-st_idx),\n",
    "        )\n",
    "      )\n",
    "logging.basicConfig(level=logging.INFO, force=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_community.vectorstores.timescalevector import TimescaleVector\n",
    "\n",
    "# Create a Timescale Vector instance from the collection of documents\n",
    "db = TimescaleVector.from_documents(\n",
    "  embedding=embed_model,\n",
    "  ids=[doc.metadata[\"id\"] for doc in docs],\n",
    "  documents=docs,\n",
    "  COLLECTION_NAME_SUMM=COLLECTION_NAME_SUMM,\n",
    "  service_url=os.environ['TIMESCALE_SERVICE_URL']\n",
    ")\n",
    "\n",
    "db.drop_index()\n",
    "db.create_index(index_type=\"tsv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read Vectorstore"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.vectorstores.timescalevector import TimescaleVector\n",
    "import os\n",
    "\n",
    "db = TimescaleVector(\n",
    "    collection_name=COLLECTION_NAME_SUMM,\n",
    "    service_url=os.environ['TIMESCALE_SERVICE_URL'],\n",
    "    embedding=embed_model,\n",
    ")\n",
    "\n",
    "# db.create_index(index_type=\"tsv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "question = \"What were the last 5 ToDos papa gave?\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Retrieve docs from DB + Add Additional Context"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# from datetime import datetime\n",
    "# start_dt = datetime(2025, 1, 1)  # Start date = Jan 1, 2025\n",
    "# end_dt = datetime.now() # End date = 30 August 2023, 22:10:35\n",
    "# td = timedelta(days=7)  # Time delta = 7 days\n",
    "#\n",
    "# Set timescale vector as a retriever and specify start and end dates via kwargs\n",
    "retriever = db.as_retriever(\n",
    "  search_type=\"similarity\",\n",
    "  search_kwargs={'k': 10}\n",
    "  # search_kwargs={\"start_date\": start_dt, \"end_date\": end_dt, 'k': 10}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "retriever.invoke(question)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_community.query_constructors.timescalevector import TimescaleVectorTranslator\n",
    "\n",
    "# Give LLM info about the metadata fields\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"DATETIME\",\n",
    "        description=\"The time the message was sent. **A high priority filter**\",\n",
    "        type=\"timestamp\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"ID\",\n",
    "        description=\"A UUID v1 generated from the timestamp of the message\",\n",
    "        type=\"uuid\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"PLATFORM\",\n",
    "        description=\"The app where the message was sent. Valid values are ['Discord', 'WhatsApp']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"CHAT\",\n",
    "        description=f\"The name of the chat room where the message was sent, will be invoked using keywords 'the chat' or 'the chats'. Valid values are [{[f'\\'{name}\\'' for name in sorted(data.CHAT.unique())]}]\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Information about message chains\"\n",
    "\n",
    "vectorstore = TimescaleVector(\n",
    "    service_url=os.environ['TIMESCALE_SERVICE_URL'],\n",
    "    embedding=embed_model,\n",
    "    collection_name=COLLECTION_NAME_SUMM\n",
    ")\n",
    "\n",
    "# Instantiate the self-query retriever from an LLM\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    ret_llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    structured_query_translator=TimescaleVectorTranslator(),\n",
    "    enable_limit=True,\n",
    "    use_original_query=True,\n",
    "    verbose=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from types import MethodType\n",
    "from logging import getLogger\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "def my_get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\n",
    "        \"\"\"Get documents relevant for a query.\n",
    "\n",
    "        Args:\n",
    "            query: string to find relevant documents for\n",
    "\n",
    "        Returns:\n",
    "            List of relevant documents\n",
    "        \"\"\"\n",
    "        structured_query = self.query_constructor.invoke(\n",
    "            {\"query\": query}, config={\"callbacks\": run_manager.get_child()}\n",
    "        )\n",
    "        if self.verbose:\n",
    "            logger.info(f\"Generated Query: {structured_query}\")\n",
    "        new_query, search_kwargs = self._prepare_query(query, structured_query)\n",
    "        # ################# BEGIN: MY INTRODUCTION #################\n",
    "        # Double the requested message count, and return at least 10\n",
    "        search_kwargs['k'] = search_kwargs.get('k', 10)\n",
    "        search_kwargs['k'] = search_kwargs['k']*2\n",
    "        if search_kwargs['k'] < 10:\n",
    "          search_kwargs['k'] = 10\n",
    "        if self.verbose:\n",
    "            logger.info(f\"Final Query: {new_query} with args {search_kwargs}\")\n",
    "        # #################  END: MY INTRODUCTION  #################\n",
    "        docs = self._get_docs_with_query(new_query, search_kwargs)\n",
    "        return docs\n",
    "\n",
    "retriever._get_relevant_documents = MethodType(my_get_relevant_documents, retriever)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "docs = retriever.invoke(question)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "docs",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def retrieve_more_context(msg_id, platform, chat, n_addl_msgs=10):\n",
    "  \"\"\"\n",
    "  Given a message with ID `msg_id`, get the `addl_msgs` preceding and following messages for context\n",
    "\n",
    "  :param msg_id: the ID of a retrieved message\n",
    "  :param platform: the platform of the retrieved message\n",
    "  :param chat: the chat of the retrieved message\n",
    "  :param n_addl_msgs: number of additional messages before and after msg `msg_id` to retrieve\n",
    "  :return: a string\n",
    "  \"\"\"\n",
    "  msg_info = data[data['MSG_ID'] == msg_id]\n",
    "\n",
    "  chat_hist = data[\n",
    "      (data['PLATFORM'] == platform) &\n",
    "      (data['CHAT'] == chat)\n",
    "  ]\n",
    "\n",
    "  context_lo = max(chat_hist.index[0], msg_info.index[0] - n_addl_msgs)\n",
    "  context_hi = min(chat_hist.index[-1], msg_info.index[0] + n_addl_msgs)\n",
    "\n",
    "  within_context_df = data[(data.index >= context_lo)&(data.index <= context_hi)].copy()\n",
    "  within_context_df['VERBOSE'] = within_context_df['PLATFORM'] + ' : ' + within_context_df['CHAT'] + '\\t' + within_context_df['DATETIME'].dt.strftime('%A %B %d, %Y %H:%M') + '\\t' + within_context_df['SENDER'] + ' ~ ' + within_context_df['MESSAGE']\n",
    "\n",
    "  return within_context_df['VERBOSE'].str.cat(sep='\\n')\n",
    "\n",
    "def retrieve_more_context_summ(msg_id, platform, chat, context_len=5):\n",
    "  \"\"\"\n",
    "  Given a message with ID `msg_id`, get the `addl_msgs` preceding and following messages for context\n",
    "\n",
    "  :param msg_id: the ID of a retrieved message\n",
    "  :param platform: the platform of the retrieved message\n",
    "  :param chat: the chat of the retrieved message\n",
    "  :param n_addl_msgs: number of additional messages before and after msg `msg_id` to retrieve\n",
    "  :param context_len: the number of messages after `msg_id` in the LLM determined context\n",
    "  :return: a string of concatenated messages, a list of metadata dicts for each message in `msg_id`'s context\n",
    "  \"\"\"\n",
    "  msg_info = data[data['MSG_ID'] == msg_id]\n",
    "\n",
    "  chat_hist = data[\n",
    "      (data['PLATFORM'] == platform) &\n",
    "      (data['CHAT'] == chat)\n",
    "  ]\n",
    "\n",
    "  addl_msgs_1 = (3, 5) # for a llm determined context of size 3 msgs, grab 5 before and after\n",
    "  addl_msgs_2 = (5, 3) # for a llm determined context of size 5 msgs, grab 3 before and after\n",
    "  # find an eqn n_addl = A*exp(-lambda*context_win_len)\n",
    "  lam = (np.log(addl_msgs_1[1]) - np.log(addl_msgs_2[1]))/(np.log(addl_msgs_2[0]) - np.log(addl_msgs_1[0]))\n",
    "  A = np.exp(np.log(addl_msgs_1[1]) + lam*np.log(addl_msgs_1[0]))\n",
    "  n_addl_msgs = int(np.ceil(A*np.exp(-lam*context_len)))\n",
    "\n",
    "  context_lo = max(chat_hist.index[0], msg_info.index[0] - n_addl_msgs)\n",
    "  context_hi = min(chat_hist.index[-1], msg_info.index[0] + context_len + n_addl_msgs)\n",
    "\n",
    "  within_context_df = data[(data.index >= context_lo)&(data.index <= context_hi)].copy()\n",
    "  within_context_df['VERBOSE'] = within_context_df['DATETIME'].dt.strftime('%A %B %d, %Y %H:%M') + '\\t' + within_context_df['SENDER'] + ' ~ ' + within_context_df['MESSAGE']\n",
    "\n",
    "  meta_dicts = []\n",
    "  for _, row in within_context_df.iterrows():\n",
    "    meta_dicts.append(\n",
    "      {\n",
    "        'SENDER': row['SENDER'],\n",
    "        'DATETIME': row['DATETIME'],\n",
    "        'MESSAGE': row['MESSAGE'],\n",
    "        'PLATFORM': row['PLATFORM'],\n",
    "        'CHAT': row['CHAT']\n",
    "      }\n",
    "    )\n",
    "\n",
    "  return within_context_df['VERBOSE'].str.cat(sep='\\n'), meta_dicts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fuller_context = [\n",
    "  (doc.metadata['MSG_ID'],\n",
    "   *retrieve_more_context_summ(doc.metadata['MSG_ID'], doc.metadata['PLATFORM'], doc.metadata['CHAT'], doc.metadata['CONTEXT_LEN'])\n",
    "  ) for doc in docs\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filter Docs w/ LLM"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "# structured_llm_grader = big_llm.with_structured_output(GradeDocuments)\n",
    "structured_llm_grader = small_llm.with_structured_output(GradeDocuments, method='json_schema')\n",
    "\n",
    "# Prompt\n",
    "# system = \"\"\"You are a grader assessing relevance of a retrieved conversation snippet to a user's question. \\n\n",
    "# If **any** of the included messages contains keyword(s) or semantic meaning related to the user's question, please grade it as relevant. \\n\n",
    "# It does not need to be a stringent test, because your goal is to filter out erroneous retrievals **not grade for sparsity**. \\n\n",
    "# Please give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "docs_to_use = []\n",
    "\n",
    "for (msg_id, msg_context, metadata) in fuller_context:\n",
    "    print(msg_context, '\\n', '-'*50)\n",
    "    res = retrieval_grader.invoke({\"question\": question, \"document\": msg_context})\n",
    "    print(res,'\\n\\n\\n')\n",
    "    if res and res.binary_score == 'yes':\n",
    "        docs_to_use.append({'MSG_ID' : msg_id, 'FULL_CONTEXT' : msg_context, 'METADATA' : metadata})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(docs_to_use)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate Result"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon the given conversation snippets.\n",
    "Use three-to-five sentences maximum and keep the answer concise.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved conversation: \\n\\n <convs>{conversations}</convs> \\n\\n User question: <question>{question}</question>\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(f\"<conv{i+1}>:\\nSource:{doc['MSG_ID']}\\nContent:{doc['FULL_CONTEXT']}\\n</conv{i+1}>\\n\" for i, doc in enumerate(docs))\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | big_llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"conversations\":format_docs(docs_to_use), \"question\": question})\n",
    "print(generation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check for Hallucinations"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in 'generation' answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        ...,\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_grader = big_llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n <facts>{documents}</facts> \\n\\n LLM generation: <generation>{generation}</generation>\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "\n",
    "response = hallucination_grader.invoke({\"documents\": format_docs(docs_to_use), \"generation\": generation})\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Highlight Used Docs"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Data model\n",
    "class HighlightDocuments(BaseModel):\n",
    "    \"\"\"Return the specific part of a document used for answering the question.\"\"\"\n",
    "\n",
    "    Source: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of alphanumeric ID of docs used to answers the question\"\n",
    "    )\n",
    "    Content: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of complete conversation contexts that answers the question\"\n",
    "    )\n",
    "    Segment: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of pointed, direct segments from used documents that answer the question\"\n",
    "    )\n",
    "\n",
    "# parser\n",
    "parser = PydanticOutputParser(pydantic_object=HighlightDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an advanced assistant for document search and retrieval. You are provided with the following:\n",
    "1. A question.\n",
    "2. A generated answer based on the question.\n",
    "3. A set of documents that were referenced in generating the answer.\n",
    "\n",
    "Your task is to identify and extract the exact inline segments from the provided documents that directly correspond to the content used to\n",
    "generate the given answer. The extracted segments must be verbatim snippets from the documents, ensuring a word-for-word match with the text\n",
    "in the provided documents.\n",
    "\n",
    "Ensure that:\n",
    "- (Important) Each segment is an exact match to a part of the document and is fully contained within the document text.\n",
    "- The relevance of each segment to the generated answer is clear and directly supports the answer provided.\n",
    "- (Important) If you didn't used the specific document don't mention it.\n",
    "\n",
    "Used documents: <docs>{documents}</docs> \\n\\n User question: <question>{question}</question> \\n\\n Generated answer: <answer>{generation}</answer>\n",
    "\n",
    "<format_instruction>\n",
    "{format_instructions}\n",
    "</format_instruction>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template= system,\n",
    "    input_variables=[\"documents\", \"question\", \"generation\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Chain\n",
    "doc_lookup = prompt | big_llm | parser\n",
    "\n",
    "# Run\n",
    "lookup_response = doc_lookup.invoke({\"documents\":format_docs(docs_to_use), \"question\": question, \"generation\": generation})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for id, source, segment in zip(lookup_response.Source, lookup_response.Content, lookup_response.Segment):\n",
    "    print(f\"ID: {id}\\nSource: {source}\\nText Segment: {segment}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
